{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN Play Generator",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPDdwz67MrABxe0wM9vdlPp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElizabethA01/RNN-Play-Generator/blob/main/RNN_Play_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RNN Play Generator**\n",
        "\n",
        "I used RNN to generate a play. I showed the RNN an example of something I wanted to recreate and it will learn how to write a version of its own. \n",
        "\n",
        "I created this using a character predictive model that will take a variable length sequence as input and predict the next character. "
      ],
      "metadata": {
        "id": "IVXWQgUyfhQ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports \n",
        "%tensorflow_version 2.x \n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CvxXROfjf9CZ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset. download the file\n",
        "\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7uAC8QzggcS",
        "outputId": "98861d1c-7479-42ff-b3db-2b1b708197d9"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n",
            "1130496/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how to load your own file\n",
        "\n",
        "# from google.colab import files\n",
        "# path_to_file = list(files.upload().keys())[0]"
      ],
      "metadata": {
        "id": "3OliPOSRgccE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read content of files\n",
        "\n",
        "#read, then decode for py 2 compat\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print ('length of text: {} characters'.format(len(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Tr4sxi-hy6a",
        "outputId": "8dbbe250-90f9-43b7-d28b-0bd1eab1a6e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# take a look at the first 250 characters in text\n",
        "\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naQ27qqUiV_w",
        "outputId": "42d06371-ec8a-4a52-8789-6119eb83b638"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# encoding\n",
        "\n",
        "# need to encode each line as a different integer.\n",
        "\n",
        "vocab = sorted(set(text))\n",
        "\n",
        "#creating a mapping from unique characters to indices\n",
        "\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "def text_to_int(text):\n",
        "  return np.array([char2idx[c] for c in text])\n",
        "\n",
        "text_as_int = text_to_int(text)"
      ],
      "metadata": {
        "id": "K3IX6o1Xietr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lets look at how part of our text is encoded\n",
        "\n",
        "print('Text:', text[:13])\n",
        "print('Encoded:', text_to_int(text[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3Aabpy6kJr9",
        "outputId": "dce0c18d-cd0c-4d69-e7e4-2a8d193d5d4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text: First Citizen\n",
            "Encoded: [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function that converts our numeric values to text\n",
        "\n",
        "def int_to_text(ints):\n",
        "  try:\n",
        "    ints = ints.numpy()\n",
        "  except:\n",
        "    pass\n",
        "  return ''.join(idx2char[ints])\n",
        "\n",
        "print(int_to_text(text_as_int[:13]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHOhql3kkZoE",
        "outputId": "3032d565-58d4-4e2b-976b-16d45b541878"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# creating training examples\n",
        "\n",
        "# pick a sequence length\n",
        "\n",
        "seq_length = 100 #length of sequency for a training example\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "#creating training exmaples/ targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n"
      ],
      "metadata": {
        "id": "-v0FcUpIk8nQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# turn the stream into batches of desired length\n",
        "\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n"
      ],
      "metadata": {
        "id": "giGe0yFwlh72"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split sequences of length 101 into input and output\n",
        "\n",
        "def split_input_target(chunk): \n",
        "  input_text = chunk[:-1] # hell\n",
        "  target_text = chunk[1:] # ello\n",
        "  return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target) # use map to apply the above function to every entry"
      ],
      "metadata": {
        "id": "fun44Yfwl0kJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in dataset.take(2):\n",
        "  print('\\n\\nEXAMPLE\\n')\n",
        "  print('INPUT')\n",
        "  print(int_to_text(x))\n",
        "  print('\\nOUPUT')\n",
        "  print(int_to_text(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqvZwwPVmUo5",
        "outputId": "c8ce5686-81b4-488a-a985-f6e9396036be"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n",
            "\n",
            "OUPUT\n",
            "irst Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You \n",
            "\n",
            "\n",
            "EXAMPLE\n",
            "\n",
            "INPUT\n",
            "are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you \n",
            "\n",
            "OUPUT\n",
            "re all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make the training batches\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "VOCAB_SIZE = len(vocab) # vocab is number of unique numbers\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "data = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "metadata": {
        "id": "qMgj-FOInC76"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Building the model**"
      ],
      "metadata": {
        "id": "urgAP2gp59eN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "                               tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                                                         batch_input_shape=[batch_size, None]),\n",
        "                               tf.keras.layers.LSTM(rnn_units,\n",
        "                                                    return_sequences=True,\n",
        "                                                    stateful=True,\n",
        "                                                    recurrent_initializer='glorot_uniform'),\n",
        "                               tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "model=build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "pEz-wdbSoEtX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abb68803-7301-45bc-c66c-66c4b8688b30"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (64, None, 256)           16640     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (64, None, 1024)          5246976   \n",
            "                                                                 \n",
            " dense (Dense)               (64, None, 65)            66625     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Creating a Loss function**"
      ],
      "metadata": {
        "id": "jSh32Ukr7dK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in data.take(1):\n",
        "  example_batch_predictions = model(input_example_batch) # ask our model for a prediction on our first batch of training data\n",
        "  print(example_batch_predictions.shape, '(batch_size, sequence_length, vocab_size)') # print out the output shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nUs8Q00D7V0V",
        "outputId": "9fabaff8-80a0-437c-8ea3-5e812c310af4"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 65) (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# the prediction is an array of 64 arrays, one for each entry in the batch\n",
        "\n",
        "print(len(example_batch_predictions))\n",
        "print(example_batch_predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yCb4Tco8JzG",
        "outputId": "c379c6ae-1286-4eef-9ca0-079b6e5087d5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-9.56329459e-05  6.22043619e-03 -5.05873526e-04 ... -3.22831329e-03\n",
            "   -1.28869235e-03  2.70821201e-03]\n",
            "  [ 4.52523492e-03  3.52056045e-03 -2.88971845e-04 ... -6.77891122e-03\n",
            "    1.76084228e-03 -1.49698101e-03]\n",
            "  [ 4.75468906e-03  2.52831541e-03  5.13948093e-04 ... -3.07715381e-03\n",
            "    2.82810396e-03  2.98377126e-03]\n",
            "  ...\n",
            "  [ 1.24665201e-02  1.39832229e-03 -8.43041483e-03 ... -2.52279919e-03\n",
            "   -3.16232070e-03 -8.37589090e-04]\n",
            "  [ 1.32429749e-02 -6.60178531e-03 -7.32389791e-03 ... -2.58095493e-03\n",
            "   -8.68461933e-03  5.00956783e-03]\n",
            "  [ 1.37334270e-02 -2.96925241e-03 -6.16753614e-03 ...  3.19698156e-04\n",
            "   -3.01624252e-03  5.21255331e-03]]\n",
            "\n",
            " [[ 2.77484627e-03  2.63897306e-03  1.49044651e-03 ...  1.07557466e-03\n",
            "    4.93761105e-03  1.98380463e-03]\n",
            "  [ 4.47391160e-03 -1.51268672e-03  1.24489819e-03 ...  1.39713602e-03\n",
            "   -5.29685430e-03  3.32107418e-03]\n",
            "  [ 3.37505736e-03 -2.73080054e-03  3.61397141e-03 ...  6.28639711e-03\n",
            "   -2.05111527e-03  3.90712777e-03]\n",
            "  ...\n",
            "  [ 6.47336571e-03 -7.13816332e-03  2.18099332e-03 ... -2.41087098e-03\n",
            "   -1.71282943e-02  4.43594484e-03]\n",
            "  [ 2.05505174e-03 -6.00651512e-03  5.11833932e-03 ... -2.82774284e-03\n",
            "   -1.92673486e-02  4.39501979e-04]\n",
            "  [-7.85909826e-04 -4.70110448e-03  6.10431284e-03 ... -6.65405672e-03\n",
            "   -2.03077458e-02 -3.02342279e-03]]\n",
            "\n",
            " [[ 2.77484627e-03  2.63897306e-03  1.49044651e-03 ...  1.07557466e-03\n",
            "    4.93761105e-03  1.98380463e-03]\n",
            "  [ 7.27555901e-03  3.04972846e-03 -3.03473906e-03 ...  1.03690277e-03\n",
            "    7.46816047e-04  3.54630896e-03]\n",
            "  [ 5.11210691e-03  2.86587002e-03 -5.93216252e-03 ...  1.49875449e-03\n",
            "   -5.66006359e-03  1.50205696e-03]\n",
            "  ...\n",
            "  [-5.90511365e-04 -5.06091211e-03  2.87169620e-04 ...  5.48152393e-03\n",
            "   -2.24144268e-03  4.07757284e-03]\n",
            "  [ 8.93232599e-03 -5.65049564e-03 -1.41410029e-03 ...  1.04227371e-03\n",
            "    1.38254749e-04  6.38800533e-03]\n",
            "  [ 9.74418689e-03 -3.63941677e-03 -3.65034823e-04 ...  4.04185645e-04\n",
            "   -2.33182218e-03  5.59240719e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.73828029e-03 -6.29567821e-03  1.06445176e-03 ... -3.68280656e-04\n",
            "   -5.57193393e-03  6.00887230e-03]\n",
            "  [-2.30980315e-03 -5.01417788e-03  5.21338033e-03 ... -1.20788463e-03\n",
            "   -9.55105387e-03  1.94755464e-03]\n",
            "  [-5.14439680e-03 -1.32237410e-03  4.94532660e-03 ... -1.08679396e-03\n",
            "   -1.15094101e-02  1.83144060e-03]\n",
            "  ...\n",
            "  [-7.40489340e-04  1.04477699e-03  3.94642865e-03 ...  6.22110348e-03\n",
            "   -8.61011352e-03  3.02112964e-03]\n",
            "  [ 1.74549641e-03 -6.21823128e-03  3.34460521e-03 ...  4.32295306e-03\n",
            "   -1.36113307e-02  7.25502567e-03]\n",
            "  [ 3.54900397e-03 -1.14911133e-02  3.05558974e-03 ...  2.64880218e-04\n",
            "   -1.38891591e-02  9.67179798e-03]]\n",
            "\n",
            " [[ 5.72816795e-03 -9.10012241e-05  1.85936759e-03 ...  7.78807444e-04\n",
            "    3.08164803e-04  8.24810658e-03]\n",
            "  [ 6.80139102e-03 -6.92871492e-03  2.43914500e-03 ...  3.02673114e-04\n",
            "   -5.90732694e-03  1.25664407e-02]\n",
            "  [ 4.14276263e-03 -5.85369347e-03  4.59489413e-03 ...  6.88485987e-03\n",
            "   -2.93228379e-03  1.00793960e-02]\n",
            "  ...\n",
            "  [-2.65882188e-03 -7.36347679e-03 -2.24880595e-03 ... -3.71254701e-03\n",
            "   -1.94123160e-04 -4.74694418e-03]\n",
            "  [ 1.87819765e-03 -2.76553491e-03  1.88648264e-04 ... -1.54687106e-04\n",
            "    2.75540887e-03 -1.63391721e-03]\n",
            "  [-3.51001672e-03 -2.36141589e-03  1.94369094e-03 ... -2.35798536e-04\n",
            "    1.84892342e-04  4.13066149e-03]]\n",
            "\n",
            " [[-2.80696014e-03  1.98368821e-03  1.73158979e-03 ... -6.48811576e-04\n",
            "   -3.61877470e-03  8.26438772e-04]\n",
            "  [ 3.76027427e-04  4.62302985e-03  6.02608081e-04 ...  4.11969703e-03\n",
            "   -6.88000303e-03  3.53472261e-03]\n",
            "  [-2.30212370e-03  2.06246506e-03  4.80346102e-03 ...  8.48460244e-04\n",
            "   -1.07227704e-02  1.02439863e-05]\n",
            "  ...\n",
            "  [ 1.46644386e-02 -1.36495857e-02  2.09885743e-03 ... -9.25715503e-05\n",
            "   -9.19671543e-03  1.26458565e-02]\n",
            "  [ 8.09537247e-03 -1.16024734e-02  5.41055156e-03 ...  6.20712381e-05\n",
            "   -1.32462773e-02  5.95190795e-03]\n",
            "  [ 5.59623493e-03 -1.44885127e-02  9.28602275e-03 ...  4.60028974e-03\n",
            "   -1.44303702e-02  5.71014453e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's examine one prediction\n",
        "pred = example_batch_predictions\n",
        "print(len(pred))\n",
        "print(pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DpyJwkKe8iYs",
        "outputId": "cdb0f38f-5b34-4077-9e9f-199ac96dc9d4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "tf.Tensor(\n",
            "[[[-9.56329459e-05  6.22043619e-03 -5.05873526e-04 ... -3.22831329e-03\n",
            "   -1.28869235e-03  2.70821201e-03]\n",
            "  [ 4.52523492e-03  3.52056045e-03 -2.88971845e-04 ... -6.77891122e-03\n",
            "    1.76084228e-03 -1.49698101e-03]\n",
            "  [ 4.75468906e-03  2.52831541e-03  5.13948093e-04 ... -3.07715381e-03\n",
            "    2.82810396e-03  2.98377126e-03]\n",
            "  ...\n",
            "  [ 1.24665201e-02  1.39832229e-03 -8.43041483e-03 ... -2.52279919e-03\n",
            "   -3.16232070e-03 -8.37589090e-04]\n",
            "  [ 1.32429749e-02 -6.60178531e-03 -7.32389791e-03 ... -2.58095493e-03\n",
            "   -8.68461933e-03  5.00956783e-03]\n",
            "  [ 1.37334270e-02 -2.96925241e-03 -6.16753614e-03 ...  3.19698156e-04\n",
            "   -3.01624252e-03  5.21255331e-03]]\n",
            "\n",
            " [[ 2.77484627e-03  2.63897306e-03  1.49044651e-03 ...  1.07557466e-03\n",
            "    4.93761105e-03  1.98380463e-03]\n",
            "  [ 4.47391160e-03 -1.51268672e-03  1.24489819e-03 ...  1.39713602e-03\n",
            "   -5.29685430e-03  3.32107418e-03]\n",
            "  [ 3.37505736e-03 -2.73080054e-03  3.61397141e-03 ...  6.28639711e-03\n",
            "   -2.05111527e-03  3.90712777e-03]\n",
            "  ...\n",
            "  [ 6.47336571e-03 -7.13816332e-03  2.18099332e-03 ... -2.41087098e-03\n",
            "   -1.71282943e-02  4.43594484e-03]\n",
            "  [ 2.05505174e-03 -6.00651512e-03  5.11833932e-03 ... -2.82774284e-03\n",
            "   -1.92673486e-02  4.39501979e-04]\n",
            "  [-7.85909826e-04 -4.70110448e-03  6.10431284e-03 ... -6.65405672e-03\n",
            "   -2.03077458e-02 -3.02342279e-03]]\n",
            "\n",
            " [[ 2.77484627e-03  2.63897306e-03  1.49044651e-03 ...  1.07557466e-03\n",
            "    4.93761105e-03  1.98380463e-03]\n",
            "  [ 7.27555901e-03  3.04972846e-03 -3.03473906e-03 ...  1.03690277e-03\n",
            "    7.46816047e-04  3.54630896e-03]\n",
            "  [ 5.11210691e-03  2.86587002e-03 -5.93216252e-03 ...  1.49875449e-03\n",
            "   -5.66006359e-03  1.50205696e-03]\n",
            "  ...\n",
            "  [-5.90511365e-04 -5.06091211e-03  2.87169620e-04 ...  5.48152393e-03\n",
            "   -2.24144268e-03  4.07757284e-03]\n",
            "  [ 8.93232599e-03 -5.65049564e-03 -1.41410029e-03 ...  1.04227371e-03\n",
            "    1.38254749e-04  6.38800533e-03]\n",
            "  [ 9.74418689e-03 -3.63941677e-03 -3.65034823e-04 ...  4.04185645e-04\n",
            "   -2.33182218e-03  5.59240719e-03]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[ 1.73828029e-03 -6.29567821e-03  1.06445176e-03 ... -3.68280656e-04\n",
            "   -5.57193393e-03  6.00887230e-03]\n",
            "  [-2.30980315e-03 -5.01417788e-03  5.21338033e-03 ... -1.20788463e-03\n",
            "   -9.55105387e-03  1.94755464e-03]\n",
            "  [-5.14439680e-03 -1.32237410e-03  4.94532660e-03 ... -1.08679396e-03\n",
            "   -1.15094101e-02  1.83144060e-03]\n",
            "  ...\n",
            "  [-7.40489340e-04  1.04477699e-03  3.94642865e-03 ...  6.22110348e-03\n",
            "   -8.61011352e-03  3.02112964e-03]\n",
            "  [ 1.74549641e-03 -6.21823128e-03  3.34460521e-03 ...  4.32295306e-03\n",
            "   -1.36113307e-02  7.25502567e-03]\n",
            "  [ 3.54900397e-03 -1.14911133e-02  3.05558974e-03 ...  2.64880218e-04\n",
            "   -1.38891591e-02  9.67179798e-03]]\n",
            "\n",
            " [[ 5.72816795e-03 -9.10012241e-05  1.85936759e-03 ...  7.78807444e-04\n",
            "    3.08164803e-04  8.24810658e-03]\n",
            "  [ 6.80139102e-03 -6.92871492e-03  2.43914500e-03 ...  3.02673114e-04\n",
            "   -5.90732694e-03  1.25664407e-02]\n",
            "  [ 4.14276263e-03 -5.85369347e-03  4.59489413e-03 ...  6.88485987e-03\n",
            "   -2.93228379e-03  1.00793960e-02]\n",
            "  ...\n",
            "  [-2.65882188e-03 -7.36347679e-03 -2.24880595e-03 ... -3.71254701e-03\n",
            "   -1.94123160e-04 -4.74694418e-03]\n",
            "  [ 1.87819765e-03 -2.76553491e-03  1.88648264e-04 ... -1.54687106e-04\n",
            "    2.75540887e-03 -1.63391721e-03]\n",
            "  [-3.51001672e-03 -2.36141589e-03  1.94369094e-03 ... -2.35798536e-04\n",
            "    1.84892342e-04  4.13066149e-03]]\n",
            "\n",
            " [[-2.80696014e-03  1.98368821e-03  1.73158979e-03 ... -6.48811576e-04\n",
            "   -3.61877470e-03  8.26438772e-04]\n",
            "  [ 3.76027427e-04  4.62302985e-03  6.02608081e-04 ...  4.11969703e-03\n",
            "   -6.88000303e-03  3.53472261e-03]\n",
            "  [-2.30212370e-03  2.06246506e-03  4.80346102e-03 ...  8.48460244e-04\n",
            "   -1.07227704e-02  1.02439863e-05]\n",
            "  ...\n",
            "  [ 1.46644386e-02 -1.36495857e-02  2.09885743e-03 ... -9.25715503e-05\n",
            "   -9.19671543e-03  1.26458565e-02]\n",
            "  [ 8.09537247e-03 -1.16024734e-02  5.41055156e-03 ...  6.20712381e-05\n",
            "   -1.32462773e-02  5.95190795e-03]\n",
            "  [ 5.59623493e-03 -1.44885127e-02  9.28602275e-03 ...  4.60028974e-03\n",
            "   -1.44303702e-02  5.71014453e-03]]], shape=(64, 100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction at first timestep\n",
        "time_step = pred[0]\n",
        "print(len(time_step))\n",
        "print(time_step)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7niBxe7e8qrJ",
        "outputId": "571e8de2-4210-4255-8807-09be519bbf1b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "tf.Tensor(\n",
            "[[-9.5632946e-05  6.2204362e-03 -5.0587353e-04 ... -3.2283133e-03\n",
            "  -1.2886924e-03  2.7082120e-03]\n",
            " [ 4.5252349e-03  3.5205605e-03 -2.8897185e-04 ... -6.7789112e-03\n",
            "   1.7608423e-03 -1.4969810e-03]\n",
            " [ 4.7546891e-03  2.5283154e-03  5.1394809e-04 ... -3.0771538e-03\n",
            "   2.8281040e-03  2.9837713e-03]\n",
            " ...\n",
            " [ 1.2466520e-02  1.3983223e-03 -8.4304148e-03 ... -2.5227992e-03\n",
            "  -3.1623207e-03 -8.3758909e-04]\n",
            " [ 1.3242975e-02 -6.6017853e-03 -7.3238979e-03 ... -2.5809549e-03\n",
            "  -8.6846193e-03  5.0095678e-03]\n",
            " [ 1.3733427e-02 -2.9692524e-03 -6.1675361e-03 ...  3.1969816e-04\n",
            "  -3.0162425e-03  5.2125533e-03]], shape=(100, 65), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# to determine the predicted character we need to sample the output distribution (pick a value based on probability)\n",
        "sampled_indices = tf.random.categorical(pred, num_samples=1)\n",
        "\n",
        "#reshape the array and convert all the integers to numbers to see the actual characters\n",
        "sampled_indices = np.reshape(sample_indices, (1,-1))[0]\n",
        "predicted_chars = int_to_text(sample_indices)\n",
        "\n",
        "predicted_chars # this is what the model predicted for training sequence \n"
      ],
      "metadata": {
        "id": "-Kb_uE059D3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a loss function that can compare the output to the expected output and give us some numeric value representing how close they were\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True) # built in keras loss function"
      ],
      "metadata": {
        "id": "t68eHooO9ncN"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Compiling the model**"
      ],
      "metadata": {
        "id": "oxci4IR1-amX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "3I6mqfuX-Ztz"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt_{epoch}')\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "ijjedFPv-vKw"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the model**"
      ],
      "metadata": {
        "id": "lKqSnp1a_TS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(data, epochs = 10, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tb0cr5LM_WX_",
        "outputId": "7cd32567-944a-4af0-e7d9-7217264bbad3"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 43s 227ms/step - loss: 2.7800\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 40s 227ms/step - loss: 2.1030\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 41s 228ms/step - loss: 1.8396\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 41s 227ms/step - loss: 1.6756\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 40s 227ms/step - loss: 1.5720\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 41s 227ms/step - loss: 1.5018\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 41s 227ms/step - loss: 1.4525\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 40s 226ms/step - loss: 1.4148\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 40s 226ms/step - loss: 1.3829\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 40s 226ms/step - loss: 1.3562\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load the model**"
      ],
      "metadata": {
        "id": "JDjKezR5_6qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)"
      ],
      "metadata": {
        "id": "McYOSJRl_6Uw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "metadata": {
        "id": "ppa2DiSy_jol"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_num = 10\n",
        "model.load_weights(tf.train.load_checkpoint('./training_checkpoints/ckpt_' + str(checkpoint_num)))\n",
        "model.build(tf.TensorShape([1,None]))"
      ],
      "metadata": {
        "id": "bpKuwl9jASNH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model, start_string):\n",
        "  num_generate = 400\n",
        "\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = 1.2\n",
        "\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "    predictions = model(input_eval)\n",
        "    predictions = tf.squeeze(predictions, 0)\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "metadata": {
        "id": "isIYv2SfA0Zb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp = input('Type a starting string: ')\n",
        "print(generate_text(model, inp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKbLILPcQdh",
        "outputId": "71bcd087-7867-436f-fa6a-e28233fc2817"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type a starting string: hello\n",
            "hellow?\n",
            "\n",
            "LARTIUS:\n",
            "No, go\n",
            "Mome thus made with furnish spicurds. Isacabrot?\n",
            "No, I, brife;--what you know you here conque more your trke!\n",
            "Seeing deaply gapes, if he match you be not madal pabriciel.\n",
            "\n",
            "VICINIO:\n",
            "Prefented, Licat,' apparel, and Villovere-time\n",
            "Greamf and will, dramove free, I pray,--Do beseech you in?\n",
            "In loateng made?\n",
            "\n",
            "LADY ANNE:\n",
            "What away! how for I selfawith! wh at most die,\n",
            "Did morthman tha\n"
          ]
        }
      ]
    }
  ]
}